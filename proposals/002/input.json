{
  "Version": "002",
  "Year": "2024",
  "Semester": "Spring",
  "project_name": "Enhancing Data Usability: Comprehensive Cleaning and Packaging of Saudi Statistical Authority's Datasets",
  "Objective": " \n            The project's goal is to improve the way we handle data from the Saudi Arabian\n            Statistical Authority, making it easier for everyone to use. We're setting up \n            a special pipeline to process and clean 22 different datasets. This process is fixable \n            and created to spot and fix various data issues, making sure the cleaned data is \n            both accurate and consistent. Our approach will \n            ensure the data we clean can be trusted and useful for many different uses, making it\n            a valuable tool for anyone who needs it.",
  "Dataset": "\n            The datasets are going to be from the Saudi Arabian Statistical Authority listing \n            the monthly average prices of various goods and services in the Kingdom of Saudi Arabia \n            for the year from 2002 to 2022, each year in a different file. The dataset includes a wide \n            range of categories such as food and beverages, meats and poultry, fish and seafood, dairy products and eggs\n            , fruits and nuts, vegetables, oils and fats, along with other categories extending to construction materials, \n            personal care, and services.\n            \n            \n            Several challenges that need to be addressed in this data. \n            These challenges include the presence of Arabic\n            letters, empty columns, and non-organized lines. \n            Addressing these issues will make the dataset\n            more accessible and usable for various analytical purposes.\n            ",
  "Rationale": "\n            This project is going to help bridge the gap between raw data availability and practical usability. By cleaning and organizing the data, we make it more accessible and useful for researchers, policy-makers, and businesses, thereby fostering data-driven decision-making and innovation.\n            ",
  "Approach": "\n            I plan on approaching this capstone through several steps:\n\n            1. Data Collection: Accessing and downloading relevant datasets from the Saudi Arabian Statistical Authority.\n            2. Data Cleaning: Creating a pipeline that identify and correct inaccuracies, remove duplicates, and standarize data formats.\n            3. Pipeline Testing: After detecting patterns, test it on other datasets, then fix and modify as you go.\n            4. Data Packaging: Organizing the cleaned data into a user-friendly format, with comprehensive documentation.\n            5. Validation: Ensuring data integrity and accuracy post-cleaning.\n            6. Final Review: A thorough review of the dataset and accompanying documentation.\n            ",
  "Timeline": "\n            This is a rough timeline for this project:  \n\n            - (1 Week) Data Collection\n            - (2 Weeks) Data Cleaning \n            - (1 Week) Pipeline Testing \n            - (4 Weeks) Data Packaging and Documentation\n            - (2 Weeks) Validation of Data Integrity \n            - (2 Weeks) Final Review and Adjustments\n            - (2 Weeks) Documentation Finalization and Submission\n            ",
  "Expected Number Students": "\n            One student\n            ",
  "Possible Issues": "\n\n        Creating a data cleaning pipeline for a complex dataset involving Arabic text,\n        empty columns, and non-organized lines presents several challenges. \n        These include handling encoding issues that can arise with Arabic characters, \n        ensuring accuracy in automated translations, dealing with missing or inconsistently\n        formatted data, and developing custom cleaning rules for diverse data types.\n        Additionally, maintaining the pipeline's efficiency and scalability, especially\n        for large datasets, requires careful design to avoid long processing times.\n        Ensuring data integrity throughout the cleaning process and adapting the\n        pipeline to accommodate changes in data source formats or cleaning requirements\n        are critical for preserving the quality and usability of the cleaned data.\n        Rigorous testing and quality assurance are essential to address these challenges,\n        necessitating a balance between automation and manual review to achieve accurate\n        and reliable outcomes.            ",
  "Proposed by": "Amjad Altuwayjiri",
  "Proposed by email": "amjadalt@gwu.edu",
  "instructor": "None",
  "instructor_email": "None",
  "github_repo": "https://github.com/Amjad-Alt/capstone_project"
}